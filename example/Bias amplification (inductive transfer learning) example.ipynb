{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "815d372f",
   "metadata": {},
   "source": [
    "# Bias Amplification by Inductive Transfer Learning\n",
    "\n",
    "This notebook guides you through the process of bias amplification by inductive transfer learning. This demonstration uses MIDRC open A1 cheset X-ray image data as the example, and provides you with the instruction on how to train and deploy the model, as well as bias visualization.\n",
    "The inductive transfer learning approach establishs a two-step transfer learning process to amplifies bias. In the first step, the AI model is trained to classify patients according to a subgroup attribute (e.g., patient sex). In the second step, the model is fine-tuned to the target clinical task. dditional control over the degree to which bias is promoted using this process can be obtained by altering the number of layers frozen during the second training step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d725bcb",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "In this example you can directly uses the partitioned data sets (training, validation and testing) from the data_preprocessing example. All these three sets are equally stratified by patient sex (male and female), race (white and black) and COVID status (positive and negative). This demonstration takes patient sex as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3606ef45",
   "metadata": {},
   "source": [
    "### First step training\n",
    "\n",
    "To amplify bias by inductive transfer learning, the first step is to train the model to classify subgroup attribute (patient sex). You can run the following cell to train two separate sex classification models:\n",
    " - Model where \"M\" (male) is associated with model classification of “1”\n",
    " - Model where \"F\" (female) is associated with model classification of “1”\n",
    "\n",
    "In this demonstration we uses *ResNet-18* as the example network architecture, and pre-trained weights trained from a contrastive self-supervised learning (CSL) approach and data from the CheXpert data. This weight file can be found under */example/* directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb1dca1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full fine tuning selected\n",
      "['patient_id', 'Path', 'M', 'F', 'White', 'Black', 'Yes', 'No']\n",
      "['patient_id', 'Path', 'M', 'F', 'White', 'Black', 'Yes', 'No']\n",
      "Training...\n",
      "EPOCH\tTR-AVG-LOSS\tVD-AUC\n",
      "> 0\t0.52643\t\t0.91901\n",
      "> 1\t0.26776\t\t0.96004\n",
      "> 2\t0.17215\t\t0.96730\n",
      "> 3\t0.09069\t\t0.97279\n",
      "> 4\t0.06606\t\t0.97341\n",
      "> 5\t0.05946\t\t0.97458\n",
      "> 6\t0.04441\t\t0.97463\n",
      "> 7\t0.04609\t\t0.97450\n",
      "> 8\t0.04231\t\t0.97473\n",
      "> 9\t0.03930\t\t0.97479\n",
      "> 10\t0.03988\t\t0.97474\n",
      "> 11\t0.03863\t\t0.97491\n",
      "> 12\t0.03612\t\t0.97491\n",
      "> 13\t0.03893\t\t0.97476\n",
      "> 14\t0.04162\t\t0.97470\n",
      "Final epoch model saved to: /gpfs_projects/yuhang.zhang/OUT/2022_CXR/test/RAND_0//M/pytorch_last_epoch_model.onnx\n",
      "Final epoch model saved to: /gpfs_projects/yuhang.zhang/OUT/2022_CXR/test/RAND_0//M/best_auc_model.onnx\n",
      "END.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: model_train.py [-h] -i INPUT_TRAIN_FILE -v VALIDATION_FILE -o\n",
      "                      OUTPUT_BASE_DIR [-d DCNN] [--freeze_up_to FREEZE_UP_TO]\n",
      "                      [--pretrained_weights PRETRAINED_WEIGHTS]\n",
      "                      [-f FINE_TUNING] [-m MOCO] [-u UPTO_FREEZE] -l LOG_PATH\n",
      "                      -p OPTIMIZER [-b BATCH_SIZE] [-n NUM_EPOCHS]\n",
      "                      [-t THREADS] [-r START_LEARNING_RATE] [-s STEP_DECAY]\n",
      "                      [--SGDmomentum SGDMOMENTUM]\n",
      "                      [--decay_every_N_epoch DECAY_EVERY_N_EPOCH]\n",
      "                      [--decay_multiplier DECAY_MULTIPLIER]\n",
      "                      [-e SAVE_EVERY_N_EPOCHS]\n",
      "                      [--bsave_valid_results_at_epochs BSAVE_VALID_RESULTS_AT_EPOCHS]\n",
      "                      [-g GPU_ID] [-c CUSTOM_CHECKPOINT_FILE]\n",
      "                      [--random_state RANDOM_STATE] [--train_task TRAIN_TASK]\n",
      "model_train.py: error: the following arguments are required: -p/--optimizer\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# first step training\n",
    "main_dir = \"/gpfs_projects/yuhang.zhang/OUT/2022_CXR/test/RAND_0\"\n",
    "task_list = [\"M\", \"F\"]\n",
    "for task in task_list:\n",
    "    %run ../src/utils/model_train.py -i \"{main_dir}/train.csv\" \\\n",
    "                                     -v \"{main_dir}/validation.csv\" \\\n",
    "                                     -o \"{main_dir}/{task}\" \\\n",
    "                                     -l \"{main_dir}/{task}/run_log.log\" \\\n",
    "                                     -c \"checkpoint_csl.pth.tar\" \\\n",
    "                                     -p \"adam\" \\\n",
    "                                     -g 0 \\\n",
    "                                     --pretrained_weights True \\\n",
    "                                     --train_task \"{task}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84333aa1",
   "metadata": {},
   "source": [
    "### Second step training\n",
    "\n",
    "The second step is to fine-tune the model from step 1 to perform target clinical task. By running the following cell, you can fine-tune these two resulted models to predict COVID status, with different number of model layers being frozen. In this step, the same training/validation sets are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed95cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second step training\n",
    "frozen_layers = [1, 14]\n",
    "for task in task_list:\n",
    "    for n in frozen_layers:\n",
    "        %run ../src/utils/model_train.py -i \"{main_dir}/train.csv\" \\\n",
    "                                         -v \"{main_dir}/validation.csv\" \\\n",
    "                                         -o \"{main_dir}/{task}/{n}_frozen_layer/\" \\\n",
    "                                         -l \"{main_dir}/{task}/{n}_frozen_layer/run_log.log\" \\\n",
    "                                         -c \"{main_dir}/{task}/checkpoint__last.pth.tar\" \\\n",
    "                                         -g 0 \\\n",
    "                                         --pretrained_weights True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2fa346",
   "metadata": {},
   "source": [
    "### Baseline model training\n",
    "\n",
    "To show the degree to which bias is amplified by this approach, a baseline model is required to present baseline bias. You can run the following cell to train the baseline. To make fair comparison, the baseline uses the same model architecture and pre-trained weights (from CSL approach), as well as the same training/validation sets. However, it will skip the first step training, and directly train to perform COVID status prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline training\n",
    "%run ../src/utils/model_train.py     -i \"{main_dir}/train.csv\" \\\n",
    "                                     -v \"{main_dir}/validation.csv\" \\\n",
    "                                     -o \"{main_dir}/baseline\" \\\n",
    "                                     -l \"{main_dir}/baseline/run_log.log\" \\\n",
    "                                     -c \"checkpoint_csl.pth.tar\" \\\n",
    "                                     -g 0 \\\n",
    "                                     --pretrained_weights True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccae196",
   "metadata": {},
   "source": [
    "## Model Inference\n",
    "\n",
    "After model training is done, you can deploy the models on the independent testing set by running the inference code below. The inference code will save prediction scores as *results__.tsv* files under the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26d520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bb5788c",
   "metadata": {},
   "source": [
    "## Bias Visualization\n",
    "\n",
    "After inference, you can analyze the model bias by running the following code. The analysis code here will calculate the subgroup **predicted prevalence** and **AUROC** , and plot these measurements with respect to training disease prevalence differences between two subgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af51fc53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
