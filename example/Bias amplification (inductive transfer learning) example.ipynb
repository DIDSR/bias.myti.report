{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e724db",
   "metadata": {},
   "source": [
    "# Bias Amplification by Inductive Transfer Learning\n",
    "\n",
    "This notebook guides you through the process of bias amplification by inductive transfer learning. This demonstration uses cheset X-ray images from the Medical Imaging & Data Resource Center (MIDRC) Open-A1 data set as the example, and provides you with the instruction on how to train and deploy the model, as well as visualize the amplified bias.\n",
    "The inductive transfer learning approach establishs a two-step transfer learning process to amplifies bias. In the first step, the AI model is trained to classify patients according to a subgroup attribute (e.g., patient sex). In the second step, the model is fine-tuned to the target clinical task. dditional control over the degree to which bias is promoted using this process can be obtained by altering the number of layers frozen during the second training step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92934b3a",
   "metadata": {},
   "source": [
    "## Data Download and Conversion\n",
    "\n",
    "(**Please skip this step if you have already done**)The example uses MIDRC Open-A1 chest X-ray dataset ([MIDRC official website](https://data.midrc.org/)), which can be accessed and downloaded by following the instruction ([link for download instruction](https://data.midrc.org/dashboard/Public/documentation/Gen3_MIDRC_GetStarted.pdf)). Several *.tsv* files that include study case, patient demography and image information can also be downloaded from the website (for your convenience, we have already included them in this repository). After data is successfully downloaded, the script below will generate *.json* data summary file, and convert dicom files to *.png* files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3987df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dicom to png file\n",
    "png_save_dir = \"/gpfs_projects/yuhang.zhang/OUT/2022_CXR/open_a1_jpeg\"\n",
    "%run ../src/utils/data_conversion.py \\\n",
    "    --save_dir \"{png_save_dir}\" \\\n",
    "    --input_file \"20221010_summary_table__open_A1.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d50147d",
   "metadata": {},
   "source": [
    "## Data Partition\n",
    "\n",
    "After data are converted to *.png* files, they needed to be properly partitioned into training, validation and testing sets. In this experiment, all the data sets are equally stratified by patient sex (male and female), race (white and black) and COVID status (positive and negative). For each patient, only 1 image is selected (only CR images are used). To accelerate the whole experiment process, only 25% of the image data is used in this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf503389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning bootstrapping\n",
      "\n",
      "Number of patients/subgroup in input summary:\n",
      "subgroup\n",
      "F-Black-No-CR      678\n",
      "F-Black-Yes-CR    1913\n",
      "F-White-No-CR     2004\n",
      "F-White-Yes-CR    1250\n",
      "M-Black-No-CR      658\n",
      "M-Black-Yes-CR    1684\n",
      "M-White-No-CR     1864\n",
      "M-White-Yes-CR    1333\n",
      "Name: patient_id, dtype: int64\n",
      "\n",
      "By patient summary of data partition\n",
      "\n",
      "split           independent_test  train  validation   All\n",
      "subgroup                                                 \n",
      "F-Black-No-CR                 49     82          33   164\n",
      "F-Black-Yes-CR                49     82          33   164\n",
      "F-White-No-CR                 49     82          33   164\n",
      "F-White-Yes-CR                49     82          33   164\n",
      "M-Black-No-CR                 49     82          33   164\n",
      "M-Black-Yes-CR                49     82          33   164\n",
      "M-White-No-CR                 49     82          33   164\n",
      "M-White-Yes-CR                49     82          33   164\n",
      "All                          392    656         264  1312\n",
      "\n",
      "DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data partition\n",
    "%run ../src/utils/data_partitions.py \\\n",
    "    --input_list \"20221010_summary_table__open_A1.json\" \\\n",
    "    --conversion_file \"/gpfs_projects/yuhang.zhang/OUT/2022_CXR/open_a1_jpeg/conversion_table.json\" \\\n",
    "    --test_size 0.3 \\\n",
    "    --validation_size 0.2 \\\n",
    "    --partition_name \"juypter_indirect_test\" \\\n",
    "    --save_dir \"/gpfs_projects/yuhang.zhang/OUT/2022_CXR/\" \\\n",
    "    --max_img_per_patient 1 \\\n",
    "    --tasks 'M' 'F' 'White' 'Black' 'Yes' 'No' \\\n",
    "    --patient_img_selection_mode \"random\" \\\n",
    "    --random_seed 1 \\\n",
    "    --subsample_rate 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e11fbc",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "After data partitioning is done, the following section will show you how to train the model to amplify the bias between subgroups. This demonstration takes patient sex as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3606ef45",
   "metadata": {},
   "source": [
    "### First step training\n",
    "\n",
    "To amplify bias by inductive transfer learning, the first step is to train the model to classify subgroup attribute (patient sex). You can run the following cell to train two separate sex classification models:\n",
    " - Model where \"M\" (male) is associated with model classification of “1”\n",
    " - Model where \"F\" (female) is associated with model classification of “1”\n",
    "\n",
    "In this demonstration we uses *ResNet-18* as the example network architecture, and pre-trained weights trained from a contrastive self-supervised learning (CSL) approach and data from the CheXpert data. This weight file can be found under */example/* directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb1dca1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start experiment...\n",
      "Full fine tuning selected\n",
      "Training for task: M\n",
      "EPOCH\tTR-AVG-LOSS\tVD-AUC\n",
      "> 0\t0.69090\t\t0.70070\n",
      "> 1\t0.66199\t\t0.79454\n",
      "> 2\t0.53337\t\t0.83947\n",
      "> 3\t0.44307\t\t0.86771\n",
      "> 4\t0.41053\t\t0.88539\n",
      "> 5\t0.38826\t\t0.89199\n",
      "> 6\t0.36295\t\t0.89560\n",
      "> 7\t0.34964\t\t0.89773\n",
      "> 8\t0.35653\t\t0.89853\n",
      "> 9\t0.34976\t\t0.89905\n",
      "Final epoch model saved to: /gpfs_projects/yuhang.zhang/OUT/2022_CXR/juypter_indirect_test/RAND_1/M/pytorch_last_epoch_model.onnx\n",
      "END.\n",
      "Start experiment...\n",
      "Full fine tuning selected\n",
      "Training for task: F\n",
      "EPOCH\tTR-AVG-LOSS\tVD-AUC\n",
      "> 0\t0.68879\t\t0.62839\n",
      "> 1\t0.65459\t\t0.73921\n",
      "> 2\t0.53719\t\t0.81847\n",
      "> 3\t0.44722\t\t0.86263\n",
      "> 4\t0.41363\t\t0.88355\n",
      "> 5\t0.38718\t\t0.89268\n",
      "> 6\t0.35694\t\t0.89710\n",
      "> 7\t0.34621\t\t0.89956\n",
      "> 8\t0.35292\t\t0.90152\n",
      "> 9\t0.34506\t\t0.90186\n",
      "Final epoch model saved to: /gpfs_projects/yuhang.zhang/OUT/2022_CXR/juypter_indirect_test/RAND_1/F/pytorch_last_epoch_model.onnx\n",
      "END.\n"
     ]
    }
   ],
   "source": [
    "# first step training\n",
    "main_dir = \"/gpfs_projects/yuhang.zhang/OUT/2022_CXR/juypter_indirect_test/RAND_1\"\n",
    "task_list = [\"M\", \"F\"]\n",
    "for task in task_list:\n",
    "    %run ../src/utils/model_train.py -i \"{main_dir}/train.csv\" \\\n",
    "                                     -v \"{main_dir}/validation.csv\" \\\n",
    "                                     -o \"{main_dir}/{task}\" \\\n",
    "                                     -l \"{main_dir}/{task}/run_log.log\" \\\n",
    "                                     -c \"checkpoint_csl.pth.tar\" \\\n",
    "                                     -p \"adam\" \\\n",
    "                                     -g 0 \\\n",
    "                                     --random_state 0 \\\n",
    "                                     --pretrained_weights True \\\n",
    "                                     --train_task \"{task}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3728d79",
   "metadata": {},
   "source": [
    "### Second step training\n",
    "\n",
    "The second step is to fine-tune the model from step 1 to perform target clinical task. During this step, the same training/validation sets are used. By running the following cell, you can fine-tune these two resulted models to predict COVID status, with different number of model layers being frozen. In this example, two different number of frozen layers are shown for both pre-trained models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed95cfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start experiment...\n",
      "Fine tuning with first 1 layers frozen\n",
      "Training for task: Yes\n",
      "EPOCH\tTR-AVG-LOSS\tVD-AUC\n",
      "> 0\t0.83723\t\t0.64107\n",
      "> 1\t0.64429\t\t0.65852\n",
      "> 2\t0.60812\t\t0.66730\n",
      "> 3\t0.57995\t\t0.66695\n",
      "> 4\t0.57941\t\t0.66954\n",
      "> 5\t0.56721\t\t0.67051\n",
      "> 6\t0.56085\t\t0.67051\n",
      "> 7\t0.56177\t\t0.67034\n",
      "> 8\t0.55529\t\t0.67045\n",
      "> 9\t0.55262\t\t0.67068\n",
      "Final epoch model saved to: /gpfs_projects/yuhang.zhang/OUT/2022_CXR/juypter_indirect_test/RAND_1/M_1_frozen_layer/pytorch_last_epoch_model.onnx\n",
      "END.\n",
      "Start experiment...\n",
      "Fine tuning with first 9 layers frozen\n",
      "Training for task: Yes\n",
      "EPOCH\tTR-AVG-LOSS\tVD-AUC\n",
      "> 0\t0.85241\t\t0.62150\n",
      "> 1\t0.66258\t\t0.64836\n",
      "> 2\t0.62356\t\t0.66523\n",
      "> 3\t0.59704\t\t0.66621\n",
      "> 4\t0.59804\t\t0.66936\n",
      "> 5\t0.58780\t\t0.67097\n",
      "> 6\t0.58370\t\t0.67103\n",
      "> 7\t0.58482\t\t0.67086\n",
      "> 8\t0.57917\t\t0.67114\n",
      "> 9\t0.57739\t\t0.67109\n",
      "Final epoch model saved to: /gpfs_projects/yuhang.zhang/OUT/2022_CXR/juypter_indirect_test/RAND_1/M_9_frozen_layer/pytorch_last_epoch_model.onnx\n",
      "END.\n",
      "Start experiment...\n",
      "Fine tuning with first 1 layers frozen\n",
      "Training for task: Yes\n",
      "EPOCH\tTR-AVG-LOSS\tVD-AUC\n",
      "> 0\t0.80825\t\t0.59108\n",
      "> 1\t0.64448\t\t0.64285\n",
      "> 2\t0.60979\t\t0.65576\n",
      "> 3\t0.56609\t\t0.65513\n",
      "> 4\t0.56417\t\t0.65444\n",
      "> 5\t0.55195\t\t0.65565\n",
      "> 6\t0.53732\t\t0.65611\n",
      "> 7\t0.54474\t\t0.65593\n",
      "> 8\t0.52970\t\t0.65559\n",
      "> 9\t0.53005\t\t0.65565\n",
      "Final epoch model saved to: /gpfs_projects/yuhang.zhang/OUT/2022_CXR/juypter_indirect_test/RAND_1/F_1_frozen_layer/pytorch_last_epoch_model.onnx\n",
      "END.\n",
      "Start experiment...\n",
      "Fine tuning with first 9 layers frozen\n",
      "Training for task: Yes\n",
      "EPOCH\tTR-AVG-LOSS\tVD-AUC\n",
      "> 0\t0.81618\t\t0.57731\n",
      "> 1\t0.65122\t\t0.63235\n",
      "> 2\t0.61656\t\t0.65261\n",
      "> 3\t0.57556\t\t0.65272\n",
      "> 4\t0.57405\t\t0.65295\n",
      "> 5\t0.56277\t\t0.65416\n",
      "> 6\t0.55032\t\t0.65456\n",
      "> 7\t0.55772\t\t0.65375\n",
      "> 8\t0.54338\t\t0.65410\n",
      "> 9\t0.54374\t\t0.65329\n",
      "Final epoch model saved to: /gpfs_projects/yuhang.zhang/OUT/2022_CXR/juypter_indirect_test/RAND_1/F_9_frozen_layer/pytorch_last_epoch_model.onnx\n",
      "END.\n"
     ]
    }
   ],
   "source": [
    "# second step training\n",
    "frozen_layers = [1, 9]\n",
    "for task in task_list:\n",
    "    for n in frozen_layers:\n",
    "        %run ../src/utils/model_train.py -i \"{main_dir}/train.csv\" \\\n",
    "                                         -v \"{main_dir}/validation.csv\" \\\n",
    "                                         -o \"{main_dir}/{task}_{n}_frozen_layer/\" \\\n",
    "                                         -l \"{main_dir}/{task}_{n}_frozen_layer/run_log.log\" \\\n",
    "                                         -c \"{main_dir}/{task}/checkpoint__last.pth.tar\" \\\n",
    "                                         -f \"partial\" \\\n",
    "                                         -p \"adam\" \\\n",
    "                                         -g 0 \\\n",
    "                                         --random_state 0 \\\n",
    "                                         --pretrained_weights True \\\n",
    "                                         --freeze_up_to {n}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2fa346",
   "metadata": {},
   "source": [
    "### Baseline model training\n",
    "\n",
    "To show the degree to which bias is amplified by this approach, a baseline model is required to present baseline bias. You can run the following cell to train the baseline. To make fair comparison, the baseline uses the same model architecture and pre-trained weights (from CSL approach), as well as the same training/validation sets. However, it will skip the first step training, and directly train to perform COVID status prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4ad99d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start experiment...\n",
      "Full fine tuning selected\n",
      "Training for task: Yes\n",
      "EPOCH\tTR-AVG-LOSS\tVD-AUC\n",
      "> 0\t0.69312\t\t0.65461\n",
      "> 1\t0.67727\t\t0.66460\n",
      "> 2\t0.63984\t\t0.67378\n",
      "> 3\t0.59041\t\t0.68084\n",
      "> 4\t0.58493\t\t0.68274\n",
      "> 5\t0.56972\t\t0.68285\n",
      "> 6\t0.55712\t\t0.68331\n",
      "> 7\t0.56109\t\t0.68279\n",
      "> 8\t0.55123\t\t0.68262\n",
      "> 9\t0.55209\t\t0.68279\n",
      "Final epoch model saved to: /gpfs_projects/yuhang.zhang/OUT/2022_CXR/juypter_indirect_test/RAND_1/baseline/pytorch_last_epoch_model.onnx\n",
      "END.\n"
     ]
    }
   ],
   "source": [
    "# baseline training\n",
    "%run ../src/utils/model_train.py     -i \"{main_dir}/train.csv\" \\\n",
    "                                     -v \"{main_dir}/validation.csv\" \\\n",
    "                                     -o \"{main_dir}/baseline\" \\\n",
    "                                     -l \"{main_dir}/baseline/run_log.log\" \\\n",
    "                                     -c \"checkpoint_csl.pth.tar\" \\\n",
    "                                     -p \"adam\" \\\n",
    "                                     -g 0 \\\n",
    "                                     --random_state 0 \\\n",
    "                                     --pretrained_weights True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e2b996",
   "metadata": {},
   "source": [
    "## Model Inference\n",
    "\n",
    "After model training is done, you can deploy the models on the independent testing set by running the following cell. The inference code will save prediction scores as *results__.tsv* files under the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e26d520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inference...\n",
      "Inferencing now ...\n",
      " There are 392 test samples in the list\n",
      " AUROC = 0.708116\n",
      " Time taken:  52.96520672100087\n",
      "END.\n",
      "Start inference...\n",
      "Inferencing now ...\n",
      " There are 392 test samples in the list\n",
      " AUROC = 0.702676\n",
      " Time taken:  27.046851437997248\n",
      "END.\n",
      "Start inference...\n",
      "Inferencing now ...\n",
      " There are 392 test samples in the list\n",
      " AUROC = 0.706320\n",
      " Time taken:  25.56717879700227\n",
      "END.\n",
      "Start inference...\n",
      "Inferencing now ...\n",
      " There are 392 test samples in the list\n",
      " AUROC = 0.712464\n",
      " Time taken:  24.98355841100056\n",
      "END.\n",
      "Start inference...\n",
      "Inferencing now ...\n",
      " There are 392 test samples in the list\n",
      " AUROC = 0.707622\n",
      " Time taken:  25.18999387599979\n",
      "END.\n"
     ]
    }
   ],
   "source": [
    "# experiment model inference\n",
    "for task in task_list:\n",
    "    for n in frozen_layers:\n",
    "        %run ../src/utils/model_inference.py \\\n",
    "            -i \"{main_dir}/independent_test.csv\" \\\n",
    "            -w \"{main_dir}/{task}_{n}_frozen_layer/pytorch_last_epoch_model.onnx\" \\\n",
    "            -g 0 \\\n",
    "            -l \"{main_dir}/{task}_{n}_frozen_layer/inference_log.log\"\n",
    "# baseline model inference\n",
    "%run ../src/utils/model_inference.py \\\n",
    "            -i \"{main_dir}/independent_test.csv\" \\\n",
    "            -w \"{main_dir}/baseline/pytorch_last_epoch_model.onnx\" \\\n",
    "            -g 0 \\\n",
    "            -l \"{main_dir}/baseline/inference_log.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a565b5",
   "metadata": {},
   "source": [
    "## Bias Visualization\n",
    "\n",
    "After inference, you can analyze the model bias by running the following code. The analysis code here will calculate the subgroup **predicted prevalence** and **AUROC** , and plot these measurements with respect to training disease prevalence differences between two subgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af51fc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start subgroup bias measurements\n",
      "\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# bias measurements and visualization\n",
    "%run ../src/utils/bias_analysis.py \\\n",
    "    -d \"{main_dir}\" \\\n",
    "    -e \"baseline\" \"M_1_frozen_layer\" \"M_9_frozen_layer\" \"F_1_frozen_layer\" \"F_9_frozen_layer\" \\\n",
    "    -a \"inductive transfer learning\" \\\n",
    "    -r \"results__.tsv\" \\\n",
    "    -i \"{main_dir}/independent_test.csv\" \\\n",
    "    -s \"sex\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
