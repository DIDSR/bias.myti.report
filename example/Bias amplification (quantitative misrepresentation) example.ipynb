{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f33222e0",
   "metadata": {},
   "source": [
    "# Bias Amplification by Quantitative Misrepresentation\n",
    "\n",
    "This notebook guides you through the process of bias amplification by quantitative misrepresentation. This demonstration uses cheset X-ray images from the Medical Imaging & Data Resource Center (MIDRC) Open-A1 data set as the example, and provides you with the instruction on how to train and deploy the model, as well as visualize the amplified bias.\n",
    "Quantitative misrepresentation (i.e., data set skew) is systematically applied to the training set to simulate different levels of selection bias. Specifically, the data used during development are selected such that the disease prevalence varies between patient subgroups. The degree to which bias is promoted can be controlled by changing the degree to which the prevalence varies between subgroups.\n",
    "\n",
    "## Sections\n",
    "\n",
    "- [1. Data Download and Conversion](#data_download)\\\n",
    "Download required data set from MIDRC and convert *dicom* to *.png* files.\n",
    "- [2. Data Partition](#data_partition)\\\n",
    "Partition data into training, validation and test sets.\n",
    "- [3. Data preprocessing](#data_preprocessing)\\\n",
    "Preprocess training and validation data sets with levels of disease prevalence in different subgroups.\n",
    "- [4. Model Training](#model_training)\\\n",
    "Train bias amplification models.\n",
    "- [5. Model Inference](#model_inference)\\\n",
    "Deploy models on the independent test set.\n",
    "- [6. Bias Visualization](#bias_visualization)\\\n",
    "Calculate measurements and visualize model bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a57817",
   "metadata": {},
   "source": [
    "<a id='data_download'></a>\n",
    "## 1. Data Download and Conversion\n",
    "\n",
    "(**Please skip this step if you have already done**)The example uses MIDRC Open-A1 chest X-ray dataset ([MIDRC official website](https://data.midrc.org/)), which can be accessed and downloaded by following the instruction ([link for download instruction](https://data.midrc.org/dashboard/Public/documentation/Gen3_MIDRC_GetStarted.pdf)). Several *.tsv* files that include study case, patient demography and image information can also be downloaded from the website (for your convenience, we have already included them in this repository). After data is successfully downloaded, the script below will generate *.json* data summary file, and convert *dicom* files to *.png* files.\n",
    "**Warning: This section may take several hours to finish.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f5a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dicom to png file\n",
    "png_save_dir = \"/gpfs_projects/yuhang.zhang/OUT/2022_CXR/open_a1_jpeg\"\n",
    "%run ../src/utils/data_conversion.py \\\n",
    "    --save_dir \"{png_save_dir}\" \\\n",
    "    --input_file \"20221010_summary_table__open_A1.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e802fe6",
   "metadata": {},
   "source": [
    "<a id='data_partition'></a>\n",
    "## 2. Data Partition\n",
    "\n",
    "After data are converted to *.png* files, they needed to be properly partitioned into training, validation and testing sets. In this experiment, all the data sets are equally stratified by patient sex (male and female), race (white and black) and COVID status (positive and negative). For each patient, only 1 image is selected. To accelerate the whole experiment process, only 50% of the entire Open-A1 dataset is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1d78c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data partition\n",
    "%run ../src/utils/data_partitions.py \\\n",
    "    --input_list \"20221010_summary_table__open_A1.json\" \\\n",
    "    --conversion_file \"/gpfs_projects/yuhang.zhang/OUT/2022_CXR/open_a1_jpeg/conversion_table.json\" \\\n",
    "    --test_size 0.3 \\\n",
    "    --validation_size 0.2 \\\n",
    "    --partition_name \"juypter_direct_test\" \\\n",
    "    --save_dir \"/gpfs_projects/yuhang.zhang/OUT/2022_CXR/\" \\\n",
    "    --max_img_per_patient 1 \\\n",
    "    --tasks 'M' 'F' 'White' 'Black' 'Yes' 'No' \\\n",
    "    --patient_img_selection_mode \"random\" \\\n",
    "    --random_seed 1 \\\n",
    "    --subsample_rate 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57825f83",
   "metadata": {},
   "source": [
    "<a id='data_preprocessing'></a>\n",
    "## 3. Data Preprocessing\n",
    "\n",
    "To amplify the model bias by quantitative misrepresentation, the first step is to vary the disease prevalence within each subgroup in the **training set** while maintaining constant overall disease prevalence and subgroup distribution. \n",
    "This demonstration takes patient sex subgroup (male and female) as an example. You can run the following code to sample the disease prevalence to 0%, 25%, 50%, 75% and 100% in \"F\" (memale) subgroup, while the disease prevalence in \"M\" (male) will be 100%, 75%, 50%, 25% and 0% repectively. Noted that training set with 50% diease prevalence in each subgroup serves as the baseline.\n",
    "The code will save resulted training set *.csv* files and renamed with subgroup disease prevalence (e.g., training set with 25% disease prevalence in female subgroup will be saved as *25FP_train.csv*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63e53231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start data split of 0.1 for F\n",
      "\n",
      "Start data split of 0.25 for F\n",
      "\n",
      "Start data split of 0.5 for F\n",
      "\n",
      "Start data split of 0.75 for F\n",
      "\n",
      "Start data split of 0.9 for F\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# manipulate subgroup disease prevalence in training/validation set\n",
    "main_dir = \"/gpfs_projects/yuhang.zhang/OUT/2022_CXR/test/RAND_0\"\n",
    "%run ../src/utils/quantitative_misrepresentation_data_process.py \\\n",
    "    --input_file \"train.csv\" \\\n",
    "    --prevalences 0 0.25 0.5 0.75 1 \\\n",
    "    --test_subgroup \"F\" \\\n",
    "    --in_dir \"{main_dir}\" \\\n",
    "    --save_dir \"{main_dir}\"\n",
    "%run ../src/utils/quantitative_misrepresentation_data_process.py \\\n",
    "    --input_file \"validation.csv\" \\\n",
    "    --prevalences 0 0.25 0.5 0.75 1 \\\n",
    "    --test_subgroup \"F\" \\\n",
    "    --in_dir \"{main_dir}\" \\\n",
    "    --save_dir \"{main_dir}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e03fb2",
   "metadata": {},
   "source": [
    "<a id='model_training'></a>\n",
    "## 4. Model Training\n",
    "\n",
    "After data preprocessing is done, you can run the following cell to train models with these different training sets. In this demonstration we uses *ResNet-18* as the example network architecture, and pre-trained weights trained from a contrastive\n",
    "self-supervised learning (CSL) approach and data from the CheXpert data. This weight file can be found under *example/* directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af6d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start experiment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-29 15:14:40.637353: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-29 15:14:40.702802: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-29 15:15:18.180719: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full fine tuning selected\n",
      "['patient_id', 'Path', 'M', 'F', 'White', 'Black', 'Yes', 'No']\n",
      "['patient_id', 'Path', 'M', 'F', 'White', 'Black', 'Yes', 'No']\n",
      "Training...\n",
      "EPOCH\tTR-AVG-LOSS\tVD-AUC\n",
      "> 0\t0.64736\t\t0.60740\n",
      "> 1\t0.47892\t\t0.58973\n",
      "> 2\t0.37559\t\t0.59448\n",
      "> 3\t0.29787\t\t0.58872\n",
      "> 4\t0.27418\t\t0.58733\n",
      "> 5\t0.25196\t\t0.58974\n",
      "> 6\t0.23189\t\t0.58815\n",
      "> 7\t0.22453\t\t0.58927\n",
      "> 8\t0.22361\t\t0.58897\n",
      "> 9\t0.21520\t\t0.58852\n",
      "> 10\t0.21679\t\t0.58867\n",
      "> 11\t0.21191\t\t0.58887\n",
      "> 12\t0.21539\t\t0.58925\n",
      "> 13\t0.21745\t\t0.58923\n",
      "> 14\t0.21056\t\t0.58958\n",
      "Final epoch model saved to: /gpfs_projects/yuhang.zhang/OUT/2022_CXR/test/RAND_0//10FP/pytorch_last_epoch_model.onnx\n",
      "Final epoch model saved to: /gpfs_projects/yuhang.zhang/OUT/2022_CXR/test/RAND_0//10FP/best_auc_model.onnx\n",
      "END.\n",
      "Start experiment...\n",
      "Full fine tuning selected\n",
      "['patient_id', 'Path', 'M', 'F', 'White', 'Black', 'Yes', 'No']\n",
      "['patient_id', 'Path', 'M', 'F', 'White', 'Black', 'Yes', 'No']\n",
      "Training...\n",
      "EPOCH\tTR-AVG-LOSS\tVD-AUC\n",
      "> 0\t0.68779\t\t0.61231\n",
      "> 1\t0.59941\t\t0.64265\n",
      "> 2\t0.52161\t\t0.63612\n",
      "> 3\t0.45530\t\t0.63837\n",
      "> 4\t0.42564\t\t0.63511\n",
      "> 5\t0.39907\t\t0.62741\n",
      "> 6\t0.37967\t\t0.62750\n",
      "> 7\t0.37197\t\t0.62690\n",
      "> 8\t0.36401\t\t0.62701\n",
      "> 9\t0.36551\t\t0.62736\n",
      "> 10\t0.35760\t\t0.62729\n",
      "> 11\t0.35987\t\t0.62678\n",
      "> 12\t0.35878\t\t0.62695\n",
      "> 13\t0.35887\t\t0.62598\n",
      "> 14\t0.35962\t\t0.62669\n",
      "Final epoch model saved to: /gpfs_projects/yuhang.zhang/OUT/2022_CXR/test/RAND_0//25FP/pytorch_last_epoch_model.onnx\n",
      "Final epoch model saved to: /gpfs_projects/yuhang.zhang/OUT/2022_CXR/test/RAND_0//25FP/best_auc_model.onnx\n",
      "END.\n",
      "Start experiment...\n",
      "Full fine tuning selected\n",
      "['patient_id', 'Path', 'M', 'F', 'White', 'Black', 'Yes', 'No']\n",
      "['patient_id', 'Path', 'M', 'F', 'White', 'Black', 'Yes', 'No']\n",
      "Training...\n",
      "EPOCH\tTR-AVG-LOSS\tVD-AUC\n",
      "> 0\t0.68544\t\t0.64365\n",
      "> 1\t0.63335\t\t0.69013\n",
      "> 2\t0.58896\t\t0.67746\n",
      "> 3\t0.52450\t\t0.68726\n",
      "> 4\t0.49679\t\t0.68039\n",
      "> 5\t0.47417\t\t0.67948\n",
      "> 6\t0.44455\t\t0.67742\n",
      "> 7\t0.43489\t\t0.67652\n",
      "> 8\t0.43201\t\t0.67604\n",
      "> 9\t0.43072\t\t0.67526\n",
      "> 10\t0.42710\t\t0.67561\n",
      "> 11\t0.42451\t\t0.67553\n",
      "> 12\t0.42533\t\t0.67498\n",
      "> 13\t0.42480\t\t0.67518\n",
      "> 14\t0.42490\t\t0.67526\n",
      "Final epoch model saved to: /gpfs_projects/yuhang.zhang/OUT/2022_CXR/test/RAND_0//50FP/pytorch_last_epoch_model.onnx\n",
      "Final epoch model saved to: /gpfs_projects/yuhang.zhang/OUT/2022_CXR/test/RAND_0//50FP/best_auc_model.onnx\n",
      "END.\n",
      "Start experiment...\n",
      "Full fine tuning selected\n",
      "['patient_id', 'Path', 'M', 'F', 'White', 'Black', 'Yes', 'No']\n",
      "['patient_id', 'Path', 'M', 'F', 'White', 'Black', 'Yes', 'No']\n",
      "Training...\n",
      "EPOCH\tTR-AVG-LOSS\tVD-AUC\n",
      "> 0\t0.67346\t\t0.59652\n",
      "> 1\t0.59968\t\t0.63254\n",
      "> 2\t0.53914\t\t0.63750\n",
      "> 3\t0.48337\t\t0.64777\n",
      "> 4\t0.46054\t\t0.64604\n",
      "> 5\t0.43865\t\t0.64595\n",
      "> 6\t0.41668\t\t0.64500\n",
      "> 7\t0.40779\t\t0.64546\n",
      "> 8\t0.40328\t\t0.64415\n",
      "> 9\t0.40034\t\t0.64510\n",
      "> 10\t0.39798\t\t0.64550\n",
      "> 11\t0.39609\t\t0.64422\n",
      "> 12\t0.40010\t\t0.64549\n",
      "> 13\t0.39159\t\t0.64487\n",
      "> 14\t0.39632\t\t0.64538\n",
      "Final epoch model saved to: /gpfs_projects/yuhang.zhang/OUT/2022_CXR/test/RAND_0//75FP/pytorch_last_epoch_model.onnx\n",
      "Final epoch model saved to: /gpfs_projects/yuhang.zhang/OUT/2022_CXR/test/RAND_0//75FP/best_auc_model.onnx\n",
      "END.\n",
      "Start experiment...\n",
      "Full fine tuning selected\n",
      "['patient_id', 'Path', 'M', 'F', 'White', 'Black', 'Yes', 'No']\n",
      "['patient_id', 'Path', 'M', 'F', 'White', 'Black', 'Yes', 'No']\n",
      "Training...\n",
      "EPOCH\tTR-AVG-LOSS\tVD-AUC\n",
      "> 0\t0.65347\t\t0.56645\n",
      "> 1\t0.49258\t\t0.58723\n",
      "> 2\t0.40354\t\t0.59491\n",
      "> 3\t0.32485\t\t0.59206\n",
      "> 4\t0.29328\t\t0.58919\n",
      "> 5\t0.27177\t\t0.58690\n",
      "> 6\t0.25063\t\t0.58774\n",
      "> 7\t0.25447\t\t0.58801\n",
      "> 8\t0.24136\t\t0.58870\n",
      "> 9\t0.23933\t\t0.58912\n",
      "> 10\t0.24511\t\t0.58855\n",
      "> 11\t0.23913\t\t0.58810\n",
      "> 12\t0.24090\t\t0.58862\n",
      "> 13\t0.23449\t\t0.58780\n"
     ]
    }
   ],
   "source": [
    "# model training\n",
    "exp_list = [\"0FP\", \"25FP\", \"50FP\", \"75FP\", \"100FP\"]\n",
    "for exp in exp_list:\n",
    "    %run ../src/utils/model_train.py -i \"{main_dir}/{exp}_train.csv\" \\\n",
    "                                     -v \"{main_dir}/validation.csv\" \\\n",
    "                                     -o \"{main_dir}/{exp}\" \\\n",
    "                                     -l \"{main_dir}/{exp}/run_log.log\" \\\n",
    "                                     -c \"checkpoint_csl.pth.tar\" \\\n",
    "                                     -p \"adam\" \\\n",
    "                                     -g 0 \\\n",
    "                                     --pretrained_weights True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e7b65c",
   "metadata": {},
   "source": [
    "<a id='model_inference'></a>\n",
    "## 5. Model Inference\n",
    "\n",
    "After model training is done, you can deploy the models on the independent testing set by running the following cell. The inference code will save prediction scores as *results__.tsv* files under the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c86c8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['patient_id', 'Path', 'M', 'F', 'White', 'Black', 'Yes', 'No']\n",
      "Inferencing now ...\n",
      " onnxruntime available providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      " onnxruntime running on: GPU\n",
      " There were 1048 ROIs in the lists\n",
      " AUROC = 0.584017\n",
      " Time taken:  175.85512603400275\n",
      "END.\n",
      "['patient_id', 'Path', 'M', 'F', 'White', 'Black', 'Yes', 'No']\n",
      "Inferencing now ...\n",
      " onnxruntime available providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      " onnxruntime running on: GPU\n",
      " There were 1048 ROIs in the lists\n",
      " AUROC = 0.639466\n",
      " Time taken:  74.80611474101897\n",
      "END.\n",
      "['patient_id', 'Path', 'M', 'F', 'White', 'Black', 'Yes', 'No']\n",
      "Inferencing now ...\n",
      " onnxruntime available providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      " onnxruntime running on: GPU\n",
      " There were 1048 ROIs in the lists\n",
      " AUROC = 0.682226\n",
      " Time taken:  76.51662474789191\n",
      "END.\n",
      "['patient_id', 'Path', 'M', 'F', 'White', 'Black', 'Yes', 'No']\n",
      "Inferencing now ...\n",
      " onnxruntime available providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      " onnxruntime running on: GPU\n",
      " There were 1048 ROIs in the lists\n",
      " AUROC = 0.670980\n",
      " Time taken:  75.40291022800375\n",
      "END.\n",
      "['patient_id', 'Path', 'M', 'F', 'White', 'Black', 'Yes', 'No']\n",
      "Inferencing now ...\n",
      " onnxruntime available providers: ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      " onnxruntime running on: GPU\n",
      " There were 1048 ROIs in the lists\n",
      " AUROC = 0.595019\n",
      " Time taken:  75.11271558399312\n",
      "END.\n"
     ]
    }
   ],
   "source": [
    "# model inference\n",
    "for exp in exp_list:\n",
    "    %run ../src/utils/model_inference.py \\\n",
    "        -i \"{main_dir}/independent_test.csv\" \\\n",
    "        -w \"{main_dir}/{exp}/pytorch_last_epoch_model.onnx\" \\\n",
    "        -g 0 \\\n",
    "        -l \"{main_dir}/{exp}/inference_log.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b5d190",
   "metadata": {},
   "source": [
    "<a id='bias_visualization'></a>\n",
    "## 6. Bias Visualization\n",
    "\n",
    "After inference, you can analyze the model bias by running the following code. The analysis code here will calculate the subgroup **predicted prevalence** and **AUROC** , and plot these measurements with respect to training disease prevalence differences between two subgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ffbe48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# metric calculation\n",
    "%run ../src/utils/bias_analysis.py \\  \n",
    "    -d \"{main_dir}\" \\\n",
    "    -e \"0FP\" \"25FP\" \"50FP\" \"75FP\" \"100FP\" \\\n",
    "    -a \"quantitative misrepresentation\" \\\n",
    "    -r \"results__.tsv\" \\\n",
    "    -i \"{main_dir}/independent_test.csv\" \\\n",
    "    -s \"sex\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
